![alt text](https://github.com/stockholm-ai/local-papers-test/raw/master/stockholm_ai_logo.16_9.png "Stockholm AI")

# Local AI Papers - Stockholm

Explain the concept paragraph here

<a href="mailto:local-papers@stockholm.ai">
    <img src="https://github.com/stockholm-ai/local-papers-test/raw/master/contact-us.png" />
</a>


## Attention

### Multi-Head Attention
by [Vaswani](mailto:vaswani@mit.com) et al. at KTH in [Attention Is All You Need](https://paperswithcode.com/paper/attention-is-all-you-need)

<img src="https://paperswithcode.com/media/methods/multi-head-attention_l1A3G7a.png" height="200px" />
Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel.
The independent attention outputs are then concatenated and linearly transformed into the expected dimension.
Intuitively, multiple attention heads allows for attending to parts of the sequence differently
(e.g. longer-term dependencies versus shorter-term dependencies).

## Semi-Supervised Learning

## Natural Language Processing
